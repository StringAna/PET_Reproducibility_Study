%
% File proposal_template.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{url}
\usepackage{booktabs}      % For better table rules
\usepackage{siunitx}       % For number alignment
\usepackage{multirow}      % For merged rows
\usepackage{array}         % For table column formatting
\usepackage{enumitem} % Required for custom list spacing
\setlist[itemize]{itemsep=0em} % Applies to all itemize environments

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Reproducing Pattern-Exploiting Training (PET): A Critical Analysis of Few-Shot Text Classification Using AG News Dataset}

\author{First Author \\
  Antara Tewary\\ 
  G01413546\\
  \texttt{atewary@gmu.edu} \\\And
  Second Author \\
  Ankit Kumar\\ 
  G01436204\\
  \texttt{akumar37@gmu.edu}\\\And
  Third Author \\
  Homa Haghighi\\ 
  G01145449\\
  \texttt{hhaghigh@gmu.edu}}

\date{}

\begin{document}
\maketitle

\section{Introduction}

\citet{schick2021exploitingclozequestionsshot} addresses the challenge of few-shot learning in NLP tasks, where only a small, labeled dataset is available. The research question is:\\
\textit{How can pre-trained language models (PLMs) like RoBERTa or BERT be adapted to excel at few-shot learning tasks using task-specific patterns and verbalizers? }\\
The paper's main goal is to explore whether reformulating inputs as cloze-style questions can improve performance on tasks such as text classification and natural language inference (NLI) under low-resource conditions. \\
\href{https://github.com/StringAna/PET_Reproducibility_Study}{\textit{\textbf{Link to our codebase}}}~\cite{pet_reproduction2024}\\
            \subsection{Task / Research Question Description} 
            Core research questions are-\\
            - Can natural language task descriptions be effectively combined with supervised learning to improve few-shot text classification?\\
            - How can we exploit the implicit knowledge in pre-trained language models through carefully designed patterns?\\
            - Is it possible to achieve better performance than standard supervised learning with as few as 10 labeled examples per class?
            \subsection{Motivation \& Limitations of existing work} 
            Existing few-shot learning methods often fail to effectively utilize pre-trained language models due to:
            \begin{itemize}[topsep=0pt]
              \item \textit{Lack of Task-Specific Adaptation}:Pre-trained models like BERT are not optimized for cloze-style tasks, which are effective for few-shot learning. 
              \item \textit{Heavy Dependence on Labeled Data}:Supervised models require large datasets, making them impractical for low-resource scenarios.
            \end{itemize} 
            The proposed approach (PET) addresses these issues by:
            \begin{itemize}[topsep=0pt]
              \item Exploiting task descriptions using patterns (templates) and verbalizers (label-to-word mappings). 
              \item Enhancing semi-supervised training through iterative refinement (iPET), which improves performance by gradually increasing labeled data. 
            \end{itemize}
            \subsection{Proposed Approach} 
            The core contribution is the Pattern-Exploiting Training (PET) framework: 
            \begin{itemize}[topsep=0pt]
              \setlength\itemsep{0em}
              \item \textit{Pattern and Verbalizer Design}:Reformulates tasks into cloze-style questions, enabling PLMs to better leverage their pre-training knowledge. 
              \item \textit{Soft Labeling}: Uses the model to annotate unlabeled data with probabilities (soft labels) to augment the training set.
              \item \textit{iPET}: An iterative method that refines the labeled dataset over multiple training cycles, leading to improved generalization. 
            \end{itemize}
            This approach enables few-shot learning by effectively combining human-readable patterns with the capabilities of large pre-trained models. 
            \subsection{Likely challenges and mitigations}
            \begin{itemize}[topsep=0pt]
              \item \textit{Computational Resources}\\Challenge: Training large models demands significant GPU power.
              \\Mitigation:Start with smaller models and subsets of patterns; use gradient accumulation.
              \item \textit{Implementation Complexity}\\Challenge:Multi-stage training with intricate interactions.\\
              Mitigation: Modular design, extensive testing, and single-pattern trials.
            \end{itemize}


\section{Related Work}
\begin{itemize}[topsep=0pt]
  \item \citet{radford2019language} introduced GPT-2, a language model capable of zero-shot learning by using task descriptions. However, it does not employ task-specific fine-tuning or semi-supervised learning, which PET utilizes to enhance few-shot performance.
  \item \citet{mccann2018natural} reframed NLP tasks as question-answering, enabling multitask learning. But it requires substantial labeled data, while PET leverages cloze-style patterns and verbalizers to use pre-trained models effectively in low-resource settings.
  \item  \citet{xie2020unsupervised}presented a semi-supervised approach using data augmentation. PET differs by leveraging cloze-style patterns and pre-trained language models, avoiding reliance on data augmentation.
  \item \citet{zeng-zubiaga-2023-active} proposes "Active PETs", a weighted approach that uses an ensemble of PET models to actively select unlabeled data for annotation to improve few-shot claim verification. PET differs from traditional few-shot learning by reformulating tasks into cloze-style questions and using semi-supervised learning through iterative refinement to enhance performance with limited labeled data.
\end{itemize}

\section{Experiments}

\subsection{Datasets}
For this assignment, we focused on AGNews dataset, and its subset of four major news categories - World, Sports, Business, and Technology/Science. The original PET paper evaluated performance over multiple datasets like Yahoo Answers and Yelp Reviews, our focused approach enabled an in-depth analysis within our limited computational and time constrains.

\subsection{Implementation} 
Our study \cite{pet_reproduction2024} builds upon the original implementation provided by the original paper, using their publicly available codebase as our foundation which ensures reproducibility and maintain direct comparability with the original work while conducting our experiments, architectural modifications, robustness analysis, and error studies.
\begin{itemize}[topsep=0pt]
  \item \textit{Iteration 1} strictly follows the paper's configuration (100 training examples, batch sizes of 2/4, single epoch) to validate the reproducibility of the reported few-shot learning results. This is our baseline and allows us to verify PET's promise in low resource scenarios as claimed in the research.
  \item \textit{Iteration 2} was done to explore beyond the paper's scope, deliberately increasing resources (200 training examples, batch sizes of 25/50, 3  epochs) to help us understand PET's behavior under enhanced conditions. This helps understand whether the orignial paper's conservative approach was optimal, or if PET's pattern-based approach could benefit from additional computational resources and training data.
  \item \textit{Iteration 3} aimed to address some limitations within the paper. \\
  -\textit{Pattern Contextualization and Robustness}:The original PET implementation used relatively simple patterns for news classification. Our enhanced pattern system introduces more natural, context-rich formulations like "What type of news is this?" and structured categorization patterns. This modification addresses a key limitation in the original paper-\textbf{the potential disconnect between pre-training objectives and downstream task formats}. With richer contextual patterns, we aim to better leverage RoBERTa's pre-trained knowledge of language structure and domain understanding.\\
  -\textit{AGNews-Specific Optimizations}:A critical observation from the first two iterations was that the generic implementation might not fully exploit task-specific characteristics. The introduction of AGNewsConfig and specialized preprocessing demonstrates our hypothesis that news classification could benefit from domain-specific handling. The addition of special tokens like [DATA], [REPORT], and [NEWS] provides the model with explicit markers of document structure, potentially important for news article classification.\\
  -\textit{Data Quality and Representation}:The enhanced preprocessing and metadata tracking represent a significant shift from the original implementation's approach to data handling. By implementing thorough text cleaning and tracking features like headline length and numerical content, we address potential noise in news data that might affect model performance. This is particularly crucial in few-shot scenarios where each training example carries significant weight in model learning.\\
  -\textit{Verbalizer Validation and Flexibility}: The introduction of verifier mechanisms for single-token verbalizers and expanded label mappings (e.g., mapping "World" news to multiple relevant tokens like "Global" and "News") addresses a subtle but important limitation in the original implementation. This change provides more robust and flexible label representation, potentially allowing the model to capture different aspects of each news category.\\
  -\textbf{Note:}These modifications maintain the same training configuration as Iteration 1 (reproducing the paper's setup), isolating the impact of architectural changes from training dynamics. This allows us to evaluate whether sophisticated pattern-verbalizer architectures and domain-specific optimizations can enhance performance even under conservative training conditions.
\end{itemize}
\subsection{Results}
\hyperref[tab:results]{Table~\ref{tab:results} displaying result comparison with the original paper.}
\begin{table*}[t]
  
  \centering
  \small
  \setlength{\tabcolsep}{4pt}
  \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}|l|c|c|c|c|}
  \hline
  \textbf{Metric} & \textbf{\begin{tabular}[c]{@{}c@{}}Paper Results\\($|T| = 100$ for PET)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Iteration 1\\($|T| = 100$)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Iteration 2\\($|T| = 200$)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Iteration 3\\($|T| = 100$)\end{tabular}} \\
  \hline
  Test Accuracy & 88.3\% $\pm$0.1 & 88\% $\pm$0.0 & 90\% $\pm$0.0 & 88\% $\pm$0.0 \\
  \hline
  Training Examples & 100 & 100 & 200 & 100 \\
  \hline
  Training Loss & Not reported & 0.0598 & 0.0667 & 0.0404 \\
  \hline
  Training Set Accuracy (Initial) & Not reported & 24\% & 19.5\% & 24\% \\
  \hline
  Training Set Accuracy (Final) & Not reported & 92\% & 94.5\% & 94\% \\
  \hline
  \multirow{3}{*}{Configuration} & RoBERTa-large & RoBERTa-large & RoBERTa-large & RoBERTa-large \\
  & Multiple patterns & Pattern ID 0 & Pattern ID 0 & Modified patterns \\
  & 1 epoch & 1 epoch & 3 epochs & 1 epoch \\
  \hline
  \end{tabular*}
  
  \caption{Comparison of experimental results with the original paper. \\Notes: $|T|$ represents the number of training examples. 
  Paper results shown are for PET method (not iPET or supervised baselines). 
  Iteration 1 and 3 directly compare with paper's $|T| = 100$ results. 
  Iteration 2 uses more training examples (200) than reported configurations in the paper.\\}
  \label{tab:results}
  \vspace{-0.8cm}
  \end{table*}
\subsection{Discussion}
The first iteration successfully reproduced the paper's results (88\% ±0.0 vs 88.3\% ±0.1) with identical training conditions, validating PET's reproducibility. Our architectural modifications in Iteration 3 improved training efficiency (loss reduction from 0.0598 to 0.0404) while maintaining accuracy. Iteration 2 achieved higher accuracy (90\%) with more resources but increased training loss (0.0667), highlighting a performance-efficiency trade-off.

Notable differences include our consistent test accuracy (±0.0 vs paper's ±0.1), likely due to our single-pattern approach versus the paper's multiple patterns. While this simplified implementation, it potentially reduced result variability. The results demonstrate PET's reproducibility and potential for enhancement, though careful consideration of resource-performance trade-offs is crucial in few-shot learning scenarios.

\subsection{Resources}
Discuss the cost of your reproduction in terms of resources: computation, time, people, development effort, communication with the authors (if applicable).


\subsection{Error Analysis}
\begin{itemize}[topsep=0pt]
  \item \textit{Training Loss vs Accuracy Trade-off}:Iteration 3's enhanced patterns achieved the lowest training loss (0.0404 vs 0.0598 in Iteration 1) without improving test accuracy (88\%), suggesting improved training efficiency doesn't necessarily translate to better generalization.
  \item \textit{Resource Scaling Efficiency}: Doubling training data and tripling epochs in Iteration 2 yielded only 2\% accuracy improvement (90\%), indicating diminishing returns in resource-performance trade-offs.
  \item \textit{Pattern Effectiveness}: While modified patterns improved training accuracy (94\% vs 92\%), test performance remained unchanged, suggesting pattern enhancement alone doesn't guarantee better generalization.
\end{itemize}
These findings indicate PET's performance bottlenecks may lie in the fundamental pattern-based approach rather than training optimization. Future work should focus on pattern ablation studies and category-specific performance analysis to better understand these limitations.

\section{Robustness Study}
Our robustness evaluation framework assessed the model's resilience to input perturbations across different iterations of our implementation, with particular focus on our best performing model (Iteration 2, 90\% accuracy). We designed two types of perturbation tests:
\begin{itemize}[topsep=0pt]
  \item \textit{Character-level Perturbation}: Random character replacements with 0.1 probability, simulating common text errors like typos and misspellings.
  \item \textit{Word-level Perturbation}: Random word removals with 0.1 probability, testing model resilience to incomplete information
\end{itemize}
We systematically evaluated robustness across different training configurations (|T| = 10, 20, 50, 100) to understand how model stability correlates with training data size.

\subsection{Results of Robustness Evaluation}
\begin{itemize}[topsep=0pt]
  \item \textit{Baseline Performance}\\ 
  - Consistent 88\% accuracy across different few-shot configurations (10-100 examples)\\
  - Matches paper's reported performance (88.3\% ±0.1) for $|T|$ = 100\\
  - Achieves 90\% accuracy with increased training data ($|T|$ = 200)
  \item \textit{Robustness Results}\\
  - Original Accuracy: 88\%\\
  - Character Noise Impact: Drops to 4\% accuracy (95.5\% performance degradation)
  - Word Dropout Impact: Complete failure (0\% accuracy)
  \item \textit{Analysis: Successful Cases}\\ 
  - Base Model Stability: The model shows remarkable consistency in performance across different training set sizes, maintaining 88\% accuracy from $|T|$ = 10 to $|T|$ = 100.\\
  - Training Efficiency: Lower training loss in Iteration 3 (0.0404) compared to Iteration 1 (0.0598) suggests improved learning efficiency with our modifications.
  \item \textit{Analysis: Critical Vulnerabilities}\\
  - Severe Character Sensitivity: The dramatic drop to 4\% accuracy under character noise indicates brittle pattern matching rather than robust semantic understanding.\\
  - Complete Word-level Failure: Zero accuracy under word dropout suggests the model lacks ability to handle incomplete information.
\end{itemize}
These results show that while our implementation improves the paper's accuracy, the model struggles with robustness to input perturbations. Strong performance on clean data (88-90\%) versus poor resilience to perturbations (0-4\%) highlights brittle dependencies in pattern-exploiting training for few-shot learning.
\subsection{Discussion} 
Our robustness analysis revealed key insights and challenges for evaluating NLP models' robustness in few-shot learning scenarios:
\begin{itemize}[topsep=0pt]
  \item \textit{Key Challenges}:\\ 
  - Pattern Dependency Trade-off: While PET's pattern-based approach enables effective few-shot learning, it creates vulnerability to input perturbations, suggesting a fundamental tension between pattern exploitation and input robustness.\\ 
  - Evaluation Limitations: The dramatic performance drop from 88\% to 0-4\% under perturbations indicates current evaluation metrics may be too binary, missing nuanced degradation patterns.
  \item \textit{Recommendations for Future Research}:\\ 
  - Pattern Design: Implement redundant patterns capturing similar semantic information and design patterns accounting for common text variations.\\
  - Testing Framework: Develop more nuanced evaluation metrics and expand perturbation types beyond character noise and word dropout. Consider domain-specific variations.
  \item \textit{Future Directions}:\\ 
  - Adaptive pattern selection based on input quality\\
  - Hybrid approaches combining pattern-based and traditional learning\\
  - Benchmarks specifically for few-shot learning robustness
\end{itemize}

\section{Workload Clarification}
\begin{itemize}
  \item \textit{Code Implementation and Reproduction}: Each team member independently implemented the code, reproduced the paper’s results, and verified consistency across implementations.  
  \item \textit{Report Writing}: Team members drafted separate sections, then collaboratively refined them into a cohesive, clear final report.  
\end{itemize}
\section{Conclusion}
Our study reproduced the key findings of Pattern-Exploiting Training (PET), validating its effectiveness for few-shot text classification. We matched the original 88\% accuracy on AGNews with 100 examples, and achieved 90\% with 200 examples. However, our robustness analysis uncovered critical vulnerabilities - the model dropped from 88\% on clean data to 4\% and 0\% under character and word perturbations. This brittleness highlights a tension between PET's pattern-exploiting and the need for robust language understanding.
The key takeaways are:
\begin{itemize}[topsep=0pt]
  \item PET is a reproducible and effective few-shot technique, but architectural refinements did not improve generalization.
  \item PET's pattern-based nature makes it highly susceptible to input perturbations, underscoring the need for more robust few-shot learning approaches.
\end{itemize}

% \section{Credits}

% This document has been adapted from the instructions
% for earlier ACL and NAACL proceedings,
% including 
% those for 
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
% NAACL 2018 by Margaret Michell and Stephanie Lukin,
% 2017/2018 (NA)ACL bibtex suggestions from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan, 
% NAACL 2017 by Margaret Mitchell, 
% ACL 2012 by Maggie Li and Michael White, 
% those from ACL 2010 by Jing-Shing Chang and Philipp Koehn, 
% those for ACL 2008 by JohannaD. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
% those for ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
% those for ACL 2002 by Eugene Charniak and Dekang Lin, 
% and earlier ACL and EACL formats.
% Those versions were written by several
% people, including John Chen, Henry S. Thompson and Donald
% Walker. Additional elements were taken from the formatting
% instructions of the \emph{International Joint Conference on Artificial
%   Intelligence} and the \emph{Conference on Computer Vision and
%   Pattern Recognition}.

\bibliographystyle{acl_natbib} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file

\end{document}
